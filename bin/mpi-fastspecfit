#!/usr/bin/env python
"""
MPI wrapper for fastspecfit

To process all tiles in the SV1 exposure list do--
  mpi-fastspecfit

To process one specific tile and night:
  mpi-fastspecfit --tile 80605 --night 20201215

"""
import pdb # for debuggin

import sys, os, glob, time, subprocess, re
import argparse
import numpy as np
from redrock.utils import nersc_login_node
from fastspecfit.external import desi

from desiutil.log import get_logger
log = get_logger()

def weighted_partition(weights, n):
    '''
    Partition `weights` into `n` groups with approximately same sum(weights)

    Args:
        weights: array-like weights
        n: number of groups

    Returns (groups, groupweights):
        * groups: list of lists of indices of weights for each group
        * groupweights: sum of weights assigned to each group

    Notes:
        similar to `redrock.utils.distribute_work`, which was written
            independently; these have not yet been compared.
        within each group, the weights are sorted largest to smallest
    '''
    sumweights = np.zeros(n, dtype=float)
    groups = list()
    for i in range(n):
        groups.append(list())
    weights = np.asarray(weights)
    for i in np.argsort(-weights):
        j = np.argmin(sumweights)
        groups[j].append(i)
        sumweights[j] += weights[i]

    return groups, np.array([np.sum(x) for x in sumweights])

def spectra2outfiles(specfiles, inprefix, outprefix, ext=None, outdir=None):
    '''
    Convert a list of input spectra files to a list of output files

    Args:
        specfiles: list of spectra filepaths
        inprefix: input file prefix, e.g. 'spectra' or 'spPlate'
        outprefix: output file prefix, e.g. 'zbest' or 'redrock'

    Options:
        ext: output extension, e.g. 'h5' (default same as input extension)
        outdir: output directory (default same as each input file directory)

    Returns:
        array of output filepaths

    Example::

        spectra2outfiles(['/a/b/blat-1.fits', '/c/d/blat-2.fits'], \
            inprefix='blat', outprefix='foo', ext='h5', outdir='/tmp/')
        --> array(['/tmp/foo-1.h5', '/tmp/foo-2.h5'], dtype='<U13')
    '''
    outfiles = list()
    for specfile in specfiles:
        dirname, basename = os.path.split(specfile)
        outfile = basename.replace(inprefix, outprefix)
        if ext is not None:
            outfile = outfile.replace('.fits', '.'+ext)
        if outdir is None:
            outfiles.append(os.path.join(dirname, outfile))
        else:
            outfiles.append(os.path.join(outdir, outfile))

    return np.array(outfiles)

def find_specfiles(reduxdir, outdir=None, prefix='zbest', outprefix='specfit',
                   avoiddir=None):
    '''
    Returns list of spectra files under reduxdir that need to be processed.

    Options:
        reduxdir: path to redux directory
        outdir: path to output directory [default to same dir as inputs]
        prefix: filename prefix, e.g. 'spectra'
        avoiddir: subdirectory *not* to traverse

    Returns:
        list of spectra files to process

    Notes:
        Recursively walks directories under `reduxdir` looking for files
        matching prefix*.fits.  Looks for redrock*.h5 and zbest*.fits in the
        same directory as each spectra file, or in outdir if specified.
    
    '''
    if avoiddir is not None:
        avoiddir = os.path.normpath(avoiddir)

    specfiles = list()
    for dirpath, dirnames, filenames in os.walk(reduxdir, followlinks=True, topdown=True):
        if os.path.normpath(dirpath) == avoiddir:
            while True:
                try:
                    dirnames.pop()
                except IndexError:
                    break

        for filename in filenames:
            if filename.startswith(prefix) and filename.endswith('.fits'):
                specfiles.append(os.path.join(dirpath, filename))

    #log.info(reduxdir, outdir, prefix, outprefix, specfiles)
    if len(specfiles) == 0:
        log.warning('no specfiles found')
        raise IOError

    # One output file per run.
    #outfiles = spectra2outfiles(specfiles, prefix, outprefix, outdir=outdir)
    outfile = os.path.join(outdir, '{}.fits'.format(outprefix))
    if os.path.exists(outfile):
        return np.array([])
    else:
        return np.array(specfiles)

    #npix = len(specfiles)
    #todo = np.ones(npix, dtype=bool)
    #for i in range(npix):
    #    if os.path.exists(outfiles[i]) and os.path.exists(specfitfiles[i]):
    #        todo[i] = False
    #return np.array(specfiles)[todo]

def group_specfiles(specfiles, maxnodes=256, comm=None):
    '''
    Group specfiles to balance runtimes

    Args:
        specfiles: list of spectra filepaths

    Options:
        maxnodes: split the spectra into this number of nodes
        comm: MPI communicator

    Returns (groups, ntargets, grouptimes):
      * groups: list of lists of indices to specfiles
      * list of number of targets per group
      * grouptimes: list of expected runtimes for each group

    '''
    import fitsio
    
    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    npix = len(specfiles)
    pixgroups = np.array_split(np.arange(npix), size)
    ntargets = np.zeros(len(pixgroups[rank]), dtype=int)
    for i, j in enumerate(pixgroups[rank]):
        zb = fitsio.read(specfiles[j], 'ZBEST')
        fm = fitsio.read(specfiles[j], 'FIBERMAP')
        _, I, _ = np.intersect1d(fm['TARGETID'], zb['TARGETID'], return_indices=True)            
        fm = fm[I]
        assert(np.all(zb['TARGETID'] == fm['TARGETID']))
        J = ((zb['Z'] > 0) * (zb['ZWARN'] == 0) * (zb['SPECTYPE'] == 'GALAXY') * 
             (fm['OBJTYPE'] == 'TGT') * (fm['FIBERSTATUS'] == 0))
        ntargets[i] = np.count_nonzero(J)

    if comm is not None:
        ntargets = comm.gather(ntargets)
        if rank == 0:
            ntargets = np.concatenate(ntargets)
        ntargets = comm.bcast(ntargets, root=0)

    runtimes = 30 + 0.4*ntargets

    # Aim for 25 minutes, but don't exceed maxnodes number of nodes.
    ntime = 25
    if comm is not None:
        numnodes = comm.size
    else:
        numnodes = min(maxnodes, int(np.ceil(np.sum(runtimes)/(ntime*60))))

    groups, grouptimes = weighted_partition(runtimes, numnodes)
    ntargets = np.array([np.sum(ntargets[ii]) for ii in groups])
    return groups, ntargets, grouptimes

def backup_logs(logfile):
    '''
    Move logfile -> logfile.0 or logfile.1 or logfile.n as needed

    TODO: make robust against logfile.abc also existing
    '''
    logfiles = glob.glob(logfile+'.*')
    newlog = logfile+'.'+str(len(logfiles))
    assert not os.path.exists(newlog)
    os.rename(logfile, newlog)
    return newlog

def plan(args, comm=None, merge=False):
    from glob import glob
    from astropy.table import Table
    
    t0 = time.time()
    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    if rank == 0:
        for key in ['FASTSPECFIT_DATA', 'FASTSPECFIT_TEMPLATES', 'DESI_ROOT', 'DUST_DIR']:
            if key not in os.environ:
                log.fatal('Required ${} environment variable not set'.format(key))
                raise EnvironmentError('Required ${} environment variable not set'.format(key))
        outdir = os.path.join(os.getenv('FASTSPECFIT_DATA'), args.specprod, 'tiles')
        if not os.path.isdir(outdir):
            os.makedirs(outdir, exist_ok=True)

        specprod_dir = os.path.join(os.getenv('DESI_ROOT'), 'spectro', 'redux', args.specprod, 'tiles')
        
        # Unless we're given a specific tile and night (useful for debugging), get
        # the list of tiles to process from the SV1 exposures table.
        expfile = '/global/cfs/cdirs/desi/users/raichoor/fiberassign-sv1/sv1-exposures.fits'
        explist = Table.read(expfile)
        explist = explist[np.argsort(explist['NIGHT'])]
        log.info('Read {} exposures from {}'.format(len(explist), expfile))
        
        if args.tile and args.night:
            keep = (explist['TILEID'].astype(str) == args.tile) * (explist['NIGHT'].astype(str) == args.night)
            explist = explist[keep]
            log.info('Keeping {} exposures from tile {} on night {}.'.format(len(explist), args.tile, args.night))
        else:
            keep = np.zeros(len(explist), bool)
            targclasses = ['BGS+MWS', 'ELG', 'QSO+LRG']
            for targclass in targclasses:
                keep = np.logical_or(keep, explist['TARGETS'] == targclass)
            explist = explist[keep]
            log.info('Keeping {} {} exposures.'.format(len(explist), ' '.join(targclasses)))

            # Make sure the data have been reduced.
            keep = np.ones(len(explist), bool)
            alltiles = explist['TILEID'].astype(str)
            for tile in set(alltiles):
                I = np.where(tile == alltiles)[0]
                allnights = explist['NIGHT'][I].astype(str)
                for night in set(allnights):
                    J = np.where(night == allnights)[0]
                    nightdir = os.path.join(specprod_dir, tile, night)
                    if not os.path.isdir(nightdir):
                        log.info('No reductions for night {}'.format(nightdir))
                        keep[I[J]] = False
                        
            explist = explist[keep]
            log.info('Keeping {} exposures with complete reductions.'.format(len(explist)))

            # Get the root directory and the set of tiles to process.
            tiles = set(explist['TILEID'].astype(str).data)
            keep = [os.path.isdir(os.path.join(specprod_dir, tile)) for tile in tiles]
            log.info('Keeping {}/{} tiles with complete reductions.'.format(np.count_nonzero(keep), len(tiles)))
            tiles = np.array(list(tiles))[keep]
            explist = explist[np.isin(explist['TILEID'].astype(str), tiles)]
            #explist = explist[:10]
            
        if args.photfit:
            outprefix = 'photfit'
        else:
            outprefix = 'specfit'

        specfiles, outfiles = [], []
        alltiles = explist['TILEID'].astype(str)
        for tile in set(alltiles):
            I = tile == alltiles
            allnights = explist['NIGHT'][I].astype(str)
            for night in set(allnights):
                nightdir = os.path.join(specprod_dir, tile, night)
                outnightdir = os.path.join(outdir, tile, night)
                if not os.path.isdir(outnightdir):
                    os.makedirs(outnightdir, exist_ok=True)
                #outprefix = '{}-{}-{}'.format(_outprefix, tile, night)                
                #_specfiles = find_specfiles(nightdir, outnightdir, prefix='zbest', outprefix=outprefix)
                _specfiles = glob(os.path.join(nightdir, 'zbest-?-*.fits'))
                if len(_specfiles) == 0:
                    log.info('No redrock redshifts in {}'.format(nightdir))
                    continue
                #_specfiles = find_specfiles(nightdir, outnightdir, prefix='zbest', outprefix=_outprefix)
                # How many have we done already?
                for specfile in _specfiles:
                    outfile = os.path.join(outnightdir, os.path.basename(specfile).replace('zbest-', '{}-'.format(outprefix)))
                    if merge:
                        if os.path.isfile(outfile):
                            specfiles.append(specfile)
                            outfiles.append(outfile)
                    else:
                        if args.overwrite or not os.path.isfile(outfile):
                            specfiles.append(specfile)
                            outfiles.append(outfile)
        if len(specfiles) > 0:
            specfiles = np.hstack(specfiles)
            outfiles = np.hstack(outfiles)
    else:
        outdir = None
        specfiles = None
        outfiles = None

    if comm:
        outdir = comm.bcast(outdir, root=0)
        specfiles = comm.bcast(specfiles, root=0)
        outfiles = comm.bcast(outfiles, root=0)
        
    if len(specfiles) == 0:
        if rank == 0:
            log.info('All files have been processed!')
        return '', list(), list(), list(), list()

    groups, ntargets, grouptimes = group_specfiles(specfiles, args.maxnodes, comm=comm)

    if args.plan and rank == 0:
        plantime = time.time() - t0
        if plantime + np.max(grouptimes) <= (30*60):
            queue = 'debug'
        else:
            queue = 'regular'

        numnodes = len(groups)

        if os.getenv('NERSC_HOST') == 'cori':
            maxproc = 64
        elif os.getenv('NERSC_HOST') == 'edison':
            maxproc = 48
        else:
            maxproc = 8

        if args.mp is None:
            args.mp = maxproc // 2

        #- scale longer if purposefullying using fewer cores (e.g. for memory)
        if args.mp < maxproc // 2:
            scale = (maxproc//2) / args.mp
            grouptimes *= scale

        jobtime = int(1.15 * (plantime + np.max(grouptimes)))
        jobhours = jobtime // 3600
        jobminutes = (jobtime - jobhours*3600) // 60
        jobseconds = jobtime - jobhours*3600 - jobminutes*60

        print('#!/bin/bash')
        print('#SBATCH -N {}'.format(numnodes))
        print('#SBATCH -q {}'.format(queue))
        if args.photfit:
            print('#SBATCH -J fastspecfit-phot')
        else:
            print('#SBATCH -J fastspecfit-spec')
        if os.getenv('NERSC_HOST') == 'cori':
            print('#SBATCH -C haswell')
        print('#SBATCH -t {:02d}:{:02d}:{:02d}'.format(jobhours, jobminutes, jobseconds))
        print()
        print('# {} pixels with {} targets'.format(len(specfiles), np.sum(ntargets)))
        ### print('# plan time {:.1f} minutes'.format(plantime / 60))
        print('# Using {} nodes in {} queue'.format(numnodes, queue))
        print('# expected rank runtimes ({:.1f}, {:.1f}, {:.1f}) min/mid/max minutes'.format(
            np.min(grouptimes)/60, np.median(grouptimes)/60, np.max(grouptimes)/60
        ))
        ibiggest = np.argmax(grouptimes)
        print('# Largest node has {} specfile(s) with {} total targets'.format(
            len(groups[ibiggest]), ntargets[ibiggest]))

        print()
        print('export OMP_NUM_THREADS=1')
        print('unset OMP_PLACES')
        print('unset OMP_PROC_BIND')
        print('export MPICH_GNI_FORK_MODE=FULLCOPY')
        print()
        print('nodes=$SLURM_JOB_NUM_NODES')
        if False:
            rrcmd = '{} --mp {} --reduxdir {}'.format(
                os.path.abspath(__file__), args.mp, args.reduxdir)
            if args.outdir is not None:
                rrcmd += ' --outdir {}'.format(os.path.abspath(args.outdir))
            print('srun -N $nodes -n $nodes -c {} {}'.format(maxproc, rrcmd))

    return outdir, specfiles, outfiles, groups, grouptimes

def merge_fastspecfit(args, comm=None):
    """Merge all the individual catalogs into a single large catalog. Runs only on
    rank 0.

    """
    import fitsio
    from astropy.table import Table
    
    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    outdir, _, outfiles, _, _ = plan(args, comm=comm, merge=True)

    out = Table(np.hstack([fitsio.read(outfile) for outfile in outfiles[:5]]))
    #if args.photfit:
        
    pdb.set_trace()

def run_fastspecfit(args, comm=None):

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    args.maxnodes = min(args.maxnodes, size)

    t0 = time.time()
    if rank == 0:
        log.info('Starting at {}'.format(time.asctime()))

    _, specfiles, outfiles, groups, grouptimes = plan(args, comm=comm)

    if rank == 0:
        log.info('Initial setup took {:.1f} sec'.format(time.time() - t0))

    sys.stdout.flush()
    if comm:
        specfiles = comm.bcast(specfiles, root=0)
        outfiles = comm.bcast(outfiles, root=0)
        groups = comm.bcast(groups, root=0)

    # all done
    if len(specfiles) == 0:
        return
        
    assert(len(groups) == size)
    assert(len(np.concatenate(groups)) == len(specfiles))

    #pixels = np.array([int(os.path.basename(os.path.dirname(x))) for x in specfiles])
    #if args.photfit:
    #    _outprefix = 'photfit'
    #else:
    #    _outprefix = 'specfit'
        
    for ii in groups[rank]:
        log.info('Rank {} started at {}'.format(rank, time.asctime()))
        sys.stdout.flush()

        cmd = 'fastspecfit-desi {} -o {} --mp {}'.format(specfiles[ii], outfiles[ii], args.mp)
        cmd += ' --ntargets 32'

        if args.exposures:
            cmd += ' --exposures'
        if args.photfit:
            cmd += ' --photfit'
        if args.solve_vdisp:
            cmd += ' --solve-vdisp'

        logfile = outfiles[ii].replace('.fits', '.log')
        assert(logfile != outfiles[ii])

        log.info('  rank {}: {}'.format(rank, cmd))
        #log.info('LOGGING to {}'.format(logfile))
        sys.stdout.flush()

        if args.dryrun:
            continue

        try:
            t1 = time.time()
            if os.path.exists(logfile):
                backup_logs(logfile)
            # memory leak?  Try making system call instead
            with open(logfile, 'w') as mylog:
                err = subprocess.call(cmd.split(), stdout=mylog, stderr=mylog)
            dt1 = time.time() - t1
            if err == 0:
                log.info('  rank {} done in {:.2f} sec'.format(rank, dt1))
                if not os.path.exists(outfiles[ii]):
                    log.warning('  rank {} missing {}'.format(rank, outfiles[ii]))
            else:
                log.warning('  rank {} broke after {:.1f} sec with error code {}'.format(rank, dt1, err))
        except Exception as err:
            log.warning('  rank {} raised an exception'.format(rank))
            import traceback
            traceback.print_exc()

    log.info('  rank {} is done'.format(rank))
    sys.stdout.flush()

    if comm is not None:
        comm.barrier()

    if rank == 0 and not args.dryrun:
        for outfile in outfiles:
            if not os.path.exists(outfile):
                log.warning('Missing {}'.format(outfile))

        log.info('All done at {}'.format(time.asctime()))
        
def main():
    """Main wrapper on fastspecfit.

    Currently only knows about SV1 observations.

    """
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--specprod', type=str, default='daily', choices=['andes', 'daily'], help='Spectroscopic production to process.')
    parser.add_argument('--tile', default=None, type=str, help='Tile number to process (default is to do all the SV1 exposures).')
    parser.add_argument('--night', default=None, type=str, help='Night to process (default is to do all the SV1 exposures).')
    parser.add_argument('--exposures', action='store_true', default=False, help='Process the individual exposures (based on EXPID).')
    
    parser.add_argument('--mp', type=int, default=1, help='Number of multiprocessing processes per MPI rank or node.')
    parser.add_argument('--maxnodes', type=int, default=256, help='maximum number of nodes to use')

    parser.add_argument('--photfit', action='store_true', help='Fit just the broadband photometry.')
    parser.add_argument('--solve-vdisp', action='store_true', help='Solve for the velocity disperion.')

    parser.add_argument('--merge', action='store_true', help='Merge all individual catalogs into one large file.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite any existing output files.')
    parser.add_argument('--plan', action='store_true', help='Plan how many nodes to use and how to distribute the targets.')
    parser.add_argument('--nompi', action='store_true', help='Do not use MPI parallelism.')
    parser.add_argument('--dryrun', action='store_true', help='Generate but do not run commands')

    args = parser.parse_args()

    if args.merge or args.nompi or nersc_login_node():
        comm = None
    else:
        try:
            from mpi4py import MPI
            comm = MPI.COMM_WORLD
        except ImportError:
            comm = None
            
    if args.merge:
        merge_fastspecfit(args, comm=comm)
    elif args.plan:
        plan(args, comm=comm)
    else:
        run_fastspecfit(args, comm=comm)

if __name__ == '__main__':
    main()
